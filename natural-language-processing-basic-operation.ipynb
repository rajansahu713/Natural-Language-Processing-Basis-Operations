{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1 align=\"center\">Natural Language Processing Basis Operations</h1>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content:\n",
    "    1. Introduction\n",
    "    2. Word Tokenization\n",
    "    3. Sentence Tokenization \n",
    "    4. Removing stop words\n",
    "    5. Stemming of text\n",
    "    6. Lemmatizing\n",
    "    7. Part of speech Tagging\n",
    "    8. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "    Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sun', 'rises', 'in', 'the', 'east']\n"
     ]
    }
   ],
   "source": [
    "#Words Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence= \"Sun rises in the east\"\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SentenceTokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sun rises in the east.', 'Sun sets in the west.']\n"
     ]
    }
   ],
   "source": [
    "#sentences Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "example_text= \"Sun rises in the east. Sun sets in the west.\"\n",
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Removing Stop words from sentence\n",
    "    Stop words are a set of commonly used words in a language. Examples- \"a\", \"the\", \"is\", \"are\" and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_text=\"This is an example showing off stop word filtration.\"\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "words = word_tokenize(example_text)\n",
    "filtered_sentence =[w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming of Text\n",
    "* Stemming is a process where words are reduced to a root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python python python python \n",
      "\n",
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "example_words= [\"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n",
    "for w in example_words:\n",
    "  print(ps.stem(w), end=\" \")\n",
    "\n",
    "print(\"\\n\")\n",
    "text=\"It is very important to be pythonly while you are pythoning with python\"\n",
    "words=word_tokenize(text)\n",
    "for t in words:\n",
    "  print(ps.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lemmatizing\n",
    "1. Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "2. Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "low\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"lowest\",pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Part of speech Tagging\n",
    "* POS Tagging simply means labeling words with their appropriate Part-Of-Speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sun', 'NNP'), ('rises', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('east', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "example_text=\"Sun rises in the east\"\n",
    "words = word_tokenize(example_text)\n",
    "def POS_Tagging():\n",
    "  try:\n",
    "    for i in words:\n",
    "      #print(words)\n",
    "      tagged =nltk.pos_tag(words) \n",
    "    print(tagged)\n",
    "  except Exception as e :\n",
    "    print(str(e))\n",
    "\n",
    "POS_Tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reference\n",
    "    1. https://www.nltk.org/\n",
    "    2. https://github.com/nltk/nltk\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h1 align=\"center\"> Thank You!!</h1>\n",
    "\n",
    "<h5 align=\"Right\"> Rajan Sahu</h5>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
